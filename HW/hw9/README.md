# HW9

## Question

<p align="middle">
  <img src="img/Q1.png" width="800" />
</p>

## Off-Policy Control with Value Function Approximation
<p align="middle">
  <img src="img/off_qlearning.png" width="800" />
</p>

## Various ways to solve the stable issue
- Double learning
- Target networks (Fixed Q-targets)
- Experience replay

## Off-Policy Stability - The Deadly Triad
- Function Approximation
- Bootstrapping
- Off-Policy Training

## Fixed Q-Targets
<p align="middle">
  <img src="img/fixed_Q.png" width="800" />
</p>

## Double_Q
<p align="middle">
  <img src="img/double_Q.png" width="800" />
</p>
<p align="middle">
  <img src="img/double_Q2.png" width="800" />
</p>

## Experience Replay
<p align="middle">
  <img src="img/experience_reply.png" width="800" />
</p>
<p align="middle">
  <img src="img/experience_reply2.png" width="800" />
</p>

## Result
<p align="middle">
  <img src="img/batch_size10.gif" width="800" />
</p>